{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesemansfield2/genai_finance_news_stock_insights_ibm/blob/main/genai_finance_usecase_IBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI use case in finance\n",
        "Use case analytizing news and financial stock prices to provide summary based on company stock.\n",
        "Use case demonstrates generative ai technology skill using genai google and platform analytics to develop gen ai models at scale and with multiple structured and unstructured data sets.\n"
      ],
      "metadata": {
        "id": "Wrz1kNdLr9qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install google gen ai"
      ],
      "metadata": {
        "id": "gx2Ieg92sWdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wu17Wtnqr1zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d6f673-bc56-401d-efee-146f2290921e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gemini api key"
      ],
      "metadata": {
        "id": "dRf1HDOAsg3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env GEMINI_API_KEY = [GOOGLE GEMINI API KEY]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w125FHBtsnCm",
        "outputId": "6a30ce85-e8e3-4837-ab6e-de94ef27b88b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GEMINI_API_KEY=[GOOGLE GEMINI API KEY]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data input; using alpha advantage illustrative news source"
      ],
      "metadata": {
        "id": "5wtRUUrZtOKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env ALPHAVANTAGE_API_KEY = [ALPHAVANTAGE API KEY]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ1qyr7gtUM2",
        "outputId": "a7336228-f5a8-42a5-d3e8-0685266d7d03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: ALPHAVANTAGE_API_KEY=[ALPHAVANTAGE API KEY]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install google gemini llm model"
      ],
      "metadata": {
        "id": "fQcZwEnltpFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain how AI works in a few words\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.2,\n",
        "        max_output_tokens=300,\n",
        "        top_p=0.9,  # Added top_p\n",
        "        top_k=40,   # Added top_k\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking\n",
        "    ),\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZz3IijFtY0A",
        "outputId": "3af846fd-df10-40c8-94d6-d4ab4cec300c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI works by **learning patterns from data to make decisions or predictions.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Company-news Q&A chatbot (robust v2, fixed)\n",
        "\n",
        "- News: Alpha Vantage NEWS_SENTIMENT\n",
        "- Ticker search: Alpha Vantage SYMBOL_SEARCH\n",
        "- LLM: Gemini (google-genai)\n",
        "\n",
        "Setup:\n",
        "  pip install google-genai requests\n",
        "  export GOOGLE_API_KEY=...\n",
        "  export ALPHAVANTAGE_API_KEY=...\n",
        "  # optional debugging\n",
        "  export DEBUG_NEWS_BOT=1\n",
        "\n",
        "Run:\n",
        "  python news_qa_bot_v2.py\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yb55F0smuHPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Company-news Q&A chatbot (robust v2, fixed)\n",
        "\n",
        "- News: Alpha Vantage NEWS_SENTIMENT\n",
        "- Ticker search: Alpha Vantage SYMBOL_SEARCH\n",
        "- LLM: Gemini (google-genai)\n",
        "\n",
        "Setup:\n",
        "  pip install google-genai requests\n",
        "  export GOOGLE_API_KEY=...\n",
        "  export ALPHAVANTAGE_API_KEY=...\n",
        "  # optional debugging\n",
        "  export DEBUG_NEWS_BOT=1\n",
        "\n",
        "Run:\n",
        "  python news_qa_bot_v2.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# --- Gemini client ---\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.5-flash\"\n",
        "AV_BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "client = genai.Client()  # reads GOOGLE_API_KEY from env\n",
        "DEBUG = os.environ.get(\"DEBUG_NEWS_BOT\") == \"1\"\n",
        "\n",
        "def dprint(*args):\n",
        "    if DEBUG:\n",
        "        print(\"[debug]\", *args)\n",
        "\n",
        "# --- Utilities ---------------------------------------------------------------\n",
        "\n",
        "def av_time(dt: datetime) -> str:\n",
        "    \"\"\"Alpha Vantage expects UTC like YYYYMMDDTHHMM.\"\"\"\n",
        "    return dt.astimezone(timezone.utc).strftime(\"%Y%m%dT%H%M\")\n",
        "\n",
        "def symbol_search_av(company: str, apikey: str, max_hits: int = 3) -> List[str]:\n",
        "    \"\"\"Return up to max_hits likely tickers for a company name using SYMBOL_SEARCH.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"SYMBOL_SEARCH\",\n",
        "        \"keywords\": company,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "    r = requests.get(AV_BASE, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    matches = data.get(\"bestMatches\", []) or []\n",
        "    tickers = []\n",
        "    for m in matches:\n",
        "        sym = (m.get(\"1. symbol\") or m.get(\"symbol\") or \"\").upper()\n",
        "        if sym and sym not in tickers:\n",
        "            tickers.append(sym)\n",
        "        if len(tickers) >= max_hits:\n",
        "            break\n",
        "    dprint(\"SYMBOL_SEARCH:\", company, \"->\", tickers)\n",
        "    return tickers\n",
        "\n",
        "# Alpha Vantage topic map (optional)\n",
        "AV_TOPICS = {\n",
        "    \"earnings\": \"earnings\",\n",
        "    \"ipo\": \"ipo\",\n",
        "    \"m&a\": \"mergers_and_acquisitions\",\n",
        "    \"merger\": \"mergers_and_acquisitions\",\n",
        "    \"acquisition\": \"mergers_and_acquisitions\",\n",
        "    \"macroeconomy\": \"economy_macro\",\n",
        "    \"inflation\": \"economy_monetary\",\n",
        "    \"interest rates\": \"economy_monetary\",\n",
        "    \"finance\": \"finance\",\n",
        "    \"technology\": \"technology\",\n",
        "    \"retail\": \"retail_wholesale\",\n",
        "    \"real estate\": \"real_estate\",\n",
        "    \"energy\": \"energy_transportation\",\n",
        "}\n",
        "\n",
        "def fetch_news_av(\n",
        "    apikey: str,\n",
        "    ticker: Optional[str] = None,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    topics: Optional[List[str]] = None,\n",
        "    limit: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch Alpha Vantage Market News & Sentiment.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"NEWS_SENTIMENT\",\n",
        "        \"apikey\": apikey,\n",
        "        \"limit\": min(limit, 1000),\n",
        "        \"sort\": \"LATEST\",\n",
        "    }\n",
        "    if ticker:\n",
        "        params[\"tickers\"] = ticker\n",
        "    if start_dt:\n",
        "        params[\"time_from\"] = av_time(start_dt)\n",
        "    if end_dt:\n",
        "        params[\"time_to\"] = av_time(end_dt)\n",
        "    if topics:\n",
        "        mapped = [AV_TOPICS[t] for t in topics if t in AV_TOPICS]\n",
        "        if mapped:\n",
        "            params[\"topics\"] = \",\".join(sorted(set(mapped)))\n",
        "\n",
        "    r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "\n",
        "    # Throttle / info messages come back as Note/Information/Error Message\n",
        "    if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "        msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "        raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "    feed = js.get(\"feed\", []) or []\n",
        "    dprint(f\"fetch_news_av(ticker={ticker}) -> {len(feed)} articles\")\n",
        "    return feed\n",
        "\n",
        "# --- Intent extraction + robust ticker detection -----------------------------\n",
        "\n",
        "UPPER_TICKER = re.compile(r\"\\b[A-Z]{1,5}(?:\\.[A-Z]{1,3})?\\b\")  # e.g., MSFT, AAPL, BRK.B\n",
        "\n",
        "def guess_tickers_from_text(text: str) -> List[str]:\n",
        "    \"\"\"Fast heuristic: pull likely tickers from uppercase tokens like MSFT, BRK.B, TSLA.\"\"\"\n",
        "    cands = [m.group(0).upper() for m in UPPER_TICKER.finditer(text)]\n",
        "    # Avoid obvious non-tickers\n",
        "    blacklist = {\"IPO\", \"EPS\", \"CEO\", \"AI\", \"CNN\", \"GDP\", \"US\", \"USA\", \"NSE\", \"BSE\"}\n",
        "    cands = [c for c in cands if c not in blacklist]\n",
        "    # Deduplicate preserving order\n",
        "    seen, out = set(), []\n",
        "    for c in cands:\n",
        "        if c not in seen:\n",
        "            seen.add(c); out.append(c)\n",
        "    dprint(\"guess_tickers_from_text:\", out)\n",
        "    return out\n",
        "\n",
        "def extract_intent_with_gemini(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Ask Gemini to return {companies, tickers, days_lookback, topics}.\n",
        "    If LLM returns non-JSON, fall back to heuristics.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You extract structured search intent for financial news questions. \"\n",
        "        \"Return ONLY JSON with keys: \"\n",
        "        \"{companies: [company names], tickers: [tickers if explicitly present], \"\n",
        "        \"days_lookback: integer (default 14), topics: [freeform words]}.\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- If the user mentions dates like 'today', 'yesterday', 'last week', convert to an integer days_lookback.\\n\"\n",
        "        \"- If no timeframe given, use 14.\\n\"\n",
        "        \"- Companies should be plain names like 'Apple', 'Reliance Industries', 'Microsoft'.\\n\"\n",
        "        \"- Topics can be words like 'earnings', 'acquisition', 'antitrust', 'AI', 'supply chain'.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=prompt,  # pass a string (not list/dicts)\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=300,\n",
        "                top_p=0.9, # Added top_p\n",
        "                top_k=40,  # Added top_k\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        data = json.loads(resp.text)\n",
        "    except Exception as e:\n",
        "        dprint(\"Gemini intent parse failed:\", e)\n",
        "        data = {\"companies\": [], \"tickers\": [], \"days_lookback\": 14, \"topics\": []}\n",
        "\n",
        "    # Heuristic backfill: if no tickers from LLM, try raw text detection\n",
        "    if not data.get(\"tickers\"):\n",
        "        data[\"tickers\"] = guess_tickers_from_text(question)\n",
        "\n",
        "    if \"days_lookback\" not in data or not isinstance(data[\"days_lookback\"], int):\n",
        "        data[\"days_lookback\"] = 14\n",
        "\n",
        "    dprint(\"intent:\", data)\n",
        "    return data\n",
        "\n",
        "# --- Formatting + QA ---------------------------------------------------------\n",
        "\n",
        "def build_context_snippets(feeds: List[Dict[str, Any]], max_items: int = 12) -> str:\n",
        "    seen_urls = set()\n",
        "    lines, count = [], 0\n",
        "    for item in feeds:\n",
        "        url = (item.get(\"url\") or \"\").strip()\n",
        "        if not url or url in seen_urls:\n",
        "            continue\n",
        "        seen_urls.add(url)\n",
        "        count += 1\n",
        "        title = (item.get(\"title\") or \"\").strip()\n",
        "        src = (item.get(\"source\") or \"\").strip()\n",
        "        when = (item.get(\"time_published\") or \"\").strip()\n",
        "        summ = (item.get(\"summary\") or \"\").strip()\n",
        "        sent_label = (item.get(\"overall_sentiment_label\") or \"\").strip()\n",
        "        sent_score = item.get(\"overall_sentiment_score\", \"\")\n",
        "        lines.append(\n",
        "            f\"[{count}] {title} — {src} — {when}\\n{url}\\n\"\n",
        "            f\"Summary: {summ}\\nSentiment: {sent_label} ({sent_score})\\n\"\n",
        "        )\n",
        "        if count >= max_items:\n",
        "            break\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news; widen window if needed\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        return f\"News fetch error: {e}\"\n",
        "\n",
        "    if not all_news:\n",
        "        return \"I couldn’t find relevant business news for that query. Try adding a company name (e.g., 'Apple earnings this week').\"\n",
        "\n",
        "    context = build_context_snippets(all_news, max_items=12)\n",
        "    qa_prompt = (\n",
        "        \"You are a financial news analyst. Use only the articles below to answer the user's question.\\n\"\n",
        "        \"If you cite, refer to items like [1], [2]. Be concise, specific, and include recent dates.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "         config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            top_p=0.9, # Added top_p\n",
        "            top_k=40,  # Added top_k\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# --- CLI ---------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlouXCtmuO9j",
        "outputId": "0696daf4-8785-4074-8e25-6544f8781a2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "IBM is considered a stable income play and a \"Bull of the Day\" as of August 25, 2025, with strong growth prospects and estimate upgrades [3, 4, 7]. The company is actively involved in the AI economy, with deep R&D and a focus on innovative AI solutions through its watsonx platform, AI-driven automation, and strategic AI partnerships [7, 9].\n",
            "\n",
            "Recent developments for IBM include:\n",
            "*   **AI Applications:** IBM is tapping into AI-driven demand with extensive AI applications [4]. Its watsonx platform aims to boost productivity and fuel enterprise transformation [9].\n",
            "*   **Quantum Computing:** IBM is a key player in quantum computing, teaming up with AMD on quantum-centric supercomputing to merge quantum and classical systems for next-gen problem-solving [6]. They are also backing quantum momentum, with analysts seeing stronger upside for D-Wave [5]. The quantum market is projected to reach $97 billion by 2035 [6].\n",
            "*   **Space AI:** IBM and NASA launched Surya, an AI model to predict solar flares, improving prediction accuracy by 16% and offering two-hour advance visual forecasts [11].\n",
            "*   **Dividend Stock:** IBM is listed among the best dividend stocks to buy now, offering reliable dividends, high yields, and solid growth prospects [3, 8, 10].\n",
            "\n",
            "While IBM has a strong hybrid cloud push, Arista Networks (ANET) shows a sharper growth edge in 2025 in the networking sector [12].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock trend analyasis chatbot use case"
      ],
      "metadata": {
        "id": "lJZkyauyubLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eZv-bVnug00",
        "outputId": "0bf7eb7f-d893-4312-ddc9-341bce6e2032"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "IBM is considered a stable income play and a \"Bull of the Day\" as of August 25, 2025, with strong growth prospects and estimate upgrades [3, 4, 7]. The company is actively involved in the AI economy, with deep R&D and a focus on innovative AI solutions through its watsonx platform, AI-driven automation, and strategic AI partnerships [7, 9].\n",
            "\n",
            "Recent developments include:\n",
            "*   **AI and Quantum Computing:** IBM is tapping into AI-driven demand and has partnered with AMD on quantum-centric supercomputing to merge quantum and classical systems [4, 6]. They are also backing quantum momentum, with analysts seeing stronger upside for D-Wave [5].\n",
            "*   **Solar Flare Prediction:** IBM and NASA launched Surya, an AI model that improves solar flare prediction accuracy by 16% and offers two-hour advance visual forecasts [11].\n",
            "*   **Dividend Stock:** IBM is listed as one of the best dividend stocks to buy now, offering reliable dividends, high yields, and solid growth prospects [3, 8, 10].\n",
            "\n",
            "While IBM's hybrid cloud push is noted, Arista Networks (ANET) is seen as having a sharper growth edge in 2025 in the networking sector [12].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock trend analysis based on data framework\n",
        "### Based on Alpha Vantage documentation, the TIME_SERIES_DAILY endpoint is used for historical daily stock data.\n",
        "#### The required parameters are:\n",
        "#### - function: TIME_SERIES_DAILY\n",
        "#### - symbol: The ticker symbol of the stock (e.g., NVDA, AAPL)\n",
        "#### - outputsize: compact (last 100 data points) or full (full historical data)\n",
        "#### - apikey: Your Alpha Vantage API key\n",
        "#### The response format is JSON and contains daily time series data with fields like '1. open', '2. high', '3. low', '4. close', '5. volume'."
      ],
      "metadata": {
        "id": "LOv6BUdQutDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fetch_stock_data_av(\n",
        "    apikey: str,\n",
        "    ticker: str,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    outputsize: str = 'compact' # 'compact' or 'full'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Fetch Alpha Vantage daily historical stock data.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY\",\n",
        "        \"symbol\": ticker,\n",
        "        \"outputsize\": outputsize,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "\n",
        "        # Handle Alpha Vantage messages\n",
        "        if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "            msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "            raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "        # Extract daily time series data\n",
        "        time_series_data = js.get(\"Time Series (Daily)\", {}) or {}\n",
        "\n",
        "        if not time_series_data:\n",
        "            dprint(f\"No daily time series data found for ticker: {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to pandas DataFrame\n",
        "        df = pd.DataFrame.from_dict(time_series_data, orient='index')\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index() # Ensure chronological order\n",
        "\n",
        "        # Filter by date range if provided\n",
        "        if start_dt:\n",
        "            df = df[df.index >= start_dt.replace(tzinfo=None)]\n",
        "        if end_dt:\n",
        "            df = df[df.index <= end_dt.replace(tzinfo=None)]\n",
        "\n",
        "        dprint(f\"fetch_stock_data_av(ticker={ticker}) -> {len(df)} rows\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise RuntimeError(f\"API request failed: {e}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error processing stock data: {e}\")\n"
      ],
      "metadata": {
        "id": "LNuY4zbbu6Wu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_stock_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Processes historical stock data to extract relevant statistics.\"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Convert columns to numeric, forcing errors to NaN\n",
        "    for col in ['1. open', '2. high', '3. low', '4. close', '5. volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values after conversion\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Calculate daily price change and percentage change\n",
        "    df['daily_change'] = df['4. close'] - df['1. open']\n",
        "    df['daily_pct_change'] = (df['daily_change'] / df['1. open']) * 100\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['MA50'] = df['4. close'].rolling(window=50).mean()\n",
        "    df['MA200'] = df['4. close'].rolling(window=200).mean()\n",
        "\n",
        "    # Identify key values\n",
        "    latest_data = df.iloc[-1]\n",
        "    highest_close = df['4. close'].max()\n",
        "    lowest_close = df['4. close'].min()\n",
        "    date_highest_close = df['4. close'].idxmax()\n",
        "    date_lowest_close = df['4. close'].idxmin()\n",
        "\n",
        "    processed_info = {\n",
        "        \"latest_close\": latest_data['4. close'],\n",
        "        \"latest_open\": latest_data['1. open'],\n",
        "        \"latest_high\": latest_data['2. high'],\n",
        "        \"latest_low\": latest_data['3. low'],\n",
        "        \"latest_volume\": latest_data['5. volume'],\n",
        "        \"latest_daily_change\": latest_data['daily_change'],\n",
        "        \"latest_daily_pct_change\": latest_data['daily_pct_change'],\n",
        "        \"highest_close_price\": highest_close,\n",
        "        \"date_of_highest_close\": date_highest_close.strftime('%Y-%m-%d'),\n",
        "        \"lowest_close_price\": lowest_close,\n",
        "        \"date_of_lowest_close\": date_of_lowest_close.strftime('%Y-%m-%d'),\n",
        "        \"current_MA50\": latest_data['MA50'] if pd.notna(latest_data['MA50']) else None,\n",
        "        \"current_MA200\": latest_data['MA200'] if pd.notna(latest_data['MA200']) else None,\n",
        "        \"data_start_date\": df.index.min().strftime('%Y-%m-%d'),\n",
        "        \"data_end_date\": df.index.max().strftime('%Y-%m-%d'),\n",
        "        \"number_of_data_points\": len(df)\n",
        "    }\n",
        "\n",
        "    return processed_info"
      ],
      "metadata": {
        "id": "nm65FFIRvBv2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch stock data\n",
        "    stock_data = {}\n",
        "    if tickers:\n",
        "        # Assuming we only process the first resolved ticker for stock data for simplicity in this step\n",
        "        try:\n",
        "            df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "            stock_data = process_stock_data(df)\n",
        "        except RuntimeError as e:\n",
        "            return f\"Stock data fetch or processing error: {e}\"\n",
        "    else:\n",
        "         return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "\n",
        "    if not stock_data:\n",
        "        return \"I couldn’t find historical stock data for that query. Try specifying a company name or ticker.\"\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2)\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst answering questions based *only* on the provided historical stock data. \"\n",
        "        \"Do NOT use any external knowledge or refer to news articles. \"\n",
        "        \"Analyze the numerical data below to answer the user's question. \"\n",
        "        \"Include specific values and date ranges mentioned in the data where relevant. \"\n",
        "        \"If the requested information cannot be found in the data, state that clearly.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text"
      ],
      "metadata": {
        "id": "3Ojaer5_vFDl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# --- Gemini client ---\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.5-flash\"\n",
        "AV_BASE = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "client = genai.Client()  # reads GOOGLE_API_KEY from env\n",
        "DEBUG = os.environ.get(\"DEBUG_NEWS_BOT\") == \"1\"\n",
        "\n",
        "def dprint(*args):\n",
        "    if DEBUG:\n",
        "        print(\"[debug]\", *args)\n",
        "\n",
        "# --- Utilities ---------------------------------------------------------------\n",
        "\n",
        "def av_time(dt: datetime) -> str:\n",
        "    \"\"\"Alpha Vantage expects UTC like YYYYMMDDTHHMM.\"\"\"\n",
        "    return dt.astimezone(timezone.utc).strftime(\"%Y%m%dT%H%M\")\n",
        "\n",
        "def symbol_search_av(company: str, apikey: str, max_hits: int = 3) -> List[str]:\n",
        "    \"\"\"Return up to max_hits likely tickers for a company name using SYMBOL_SEARCH.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"SYMBOL_SEARCH\",\n",
        "        \"keywords\": company,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "    r = requests.get(AV_BASE, params=params, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    matches = data.get(\"bestMatches\", []) or []\n",
        "    tickers = []\n",
        "    for m in matches:\n",
        "        sym = (m.get(\"1. symbol\") or m.get(\"symbol\") or \"\").upper()\n",
        "        if sym and sym not in tickers:\n",
        "            tickers.append(sym)\n",
        "        if len(tickers) >= max_hits:\n",
        "            break\n",
        "    dprint(\"SYMBOL_SEARCH:\", company, \"->\", tickers)\n",
        "    return tickers\n",
        "\n",
        "# Alpha Vantage topic map (optional)\n",
        "AV_TOPICS = {\n",
        "    \"earnings\": \"earnings\",\n",
        "    \"ipo\": \"ipo\",\n",
        "    \"m&a\": \"mergers_and_acquisitions\",\n",
        "    \"merger\": \"mergers_and_acquisitions\",\n",
        "    \"acquisition\": \"mergers_and_acquisitions\",\n",
        "    \"macroeconomy\": \"economy_macro\",\n",
        "    \"inflation\": \"economy_monetary\",\n",
        "    \"interest rates\": \"economy_monetary\",\n",
        "    \"finance\": \"finance\",\n",
        "    \"technology\": \"technology\",\n",
        "    \"retail\": \"retail_wholesale\",\n",
        "    \"real estate\": \"real_estate\",\n",
        "    \"energy\": \"energy_transportation\",\n",
        "}\n",
        "\n",
        "def fetch_news_av(\n",
        "    apikey: str,\n",
        "    ticker: Optional[str] = None,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    topics: Optional[List[str]] = None,\n",
        "    limit: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch Alpha Vantage Market News & Sentiment.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"NEWS_SENTIMENT\",\n",
        "        \"apikey\": apikey,\n",
        "        \"limit\": min(limit, 1000),\n",
        "        \"sort\": \"LATEST\",\n",
        "    }\n",
        "    if ticker:\n",
        "        params[\"tickers\"] = ticker\n",
        "    if start_dt:\n",
        "        params[\"time_from\"] = av_time(start_dt)\n",
        "    if end_dt:\n",
        "        params[\"time_to\"] = av_time(end_dt)\n",
        "    if topics:\n",
        "        mapped = [AV_TOPICS[t] for t in topics if t in AV_TOPICS]\n",
        "        if mapped:\n",
        "            params[\"topics\"] = \",\".join(sorted(set(mapped)))\n",
        "\n",
        "    r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "\n",
        "    # Throttle / info messages come back as Note/Information/Error Message\n",
        "    if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "        msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "        raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "    feed = js.get(\"feed\", []) or []\n",
        "    dprint(f\"fetch_news_av(ticker={ticker}) -> {len(feed)} articles\")\n",
        "    return feed\n",
        "\n",
        "# --- Intent extraction + robust ticker detection -----------------------------\n",
        "\n",
        "UPPER_TICKER = re.compile(r\"\\b[A-Z]{1,5}(?:\\.[A-Z]{1,3})?\\b\")  # e.g., MSFT, AAPL, BRK.B\n",
        "\n",
        "def guess_tickers_from_text(text: str) -> List[str]:\n",
        "    \"\"\"Fast heuristic: pull likely tickers from uppercase tokens like MSFT, BRK.B, TSLA.\"\"\"\n",
        "    cands = [m.group(0).upper() for m in UPPER_TICKER.finditer(text)]\n",
        "    # Avoid obvious non-tickers\n",
        "    blacklist = {\"IPO\", \"EPS\", \"CEO\", \"AI\", \"CNN\", \"GDP\", \"US\", \"USA\", \"NSE\", \"BSE\"}\n",
        "    cands = [c for c in cands if c not in blacklist]\n",
        "    # Deduplicate preserving order\n",
        "    seen, out = set(), []\n",
        "    for c in cands:\n",
        "        if c not in seen:\n",
        "            seen.add(c); out.append(c)\n",
        "    dprint(\"guess_tickers_from_text:\", out)\n",
        "    return out\n",
        "\n",
        "def extract_intent_with_gemini(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Ask Gemini to return {companies, tickers, days_lookback, topics}.\n",
        "    If LLM returns non-JSON, fall back to heuristics.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You extract structured search intent for financial news questions. \"\n",
        "        \"Return ONLY JSON with keys: \"\n",
        "        \"{companies: [company names], tickers: [tickers if explicitly present], \"\n",
        "        \"days_lookback: integer (default 14), topics: [freeform words]}.\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- If the user mentions dates like 'today', 'yesterday', 'last week', convert to an integer days_lookback.\\n\"\n",
        "        \"- If no timeframe given, use 14.\\n\"\n",
        "        \"- Companies should be plain names like 'Apple', 'Reliance Industries', 'Microsoft'.\\n\"\n",
        "        \"- Topics can be words like 'earnings', 'acquisition', 'antitrust', 'AI', 'supply chain'.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=prompt,  # pass a string (not list/dicts)\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=300,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        data = json.loads(resp.text)\n",
        "    except Exception as e:\n",
        "        dprint(\"Gemini intent parse failed:\", e)\n",
        "        data = {\"companies\": [], \"tickers\": [], \"days_lookback\": 14, \"topics\": []}\n",
        "\n",
        "    # Heuristic backfill: if no tickers from LLM, try raw text detection\n",
        "    if not data.get(\"tickers\"):\n",
        "        data[\"tickers\"] = guess_tickers_from_text(question)\n",
        "\n",
        "    if \"days_lookback\" not in data or not isinstance(data[\"days_lookback\"], int):\n",
        "        data[\"days_lookback\"] = 14\n",
        "\n",
        "    dprint(\"intent:\", data)\n",
        "    return data\n",
        "\n",
        "# --- Formatting + QA ---------------------------------------------------------\n",
        "\n",
        "def build_context_snippets(feeds: List[Dict[str, Any]], max_items: int = 12) -> str:\n",
        "    seen_urls = set()\n",
        "    lines, count = [], 0\n",
        "    for item in feeds:\n",
        "        url = (item.get(\"url\") or \"\").strip()\n",
        "        if not url or url in seen_urls:\n",
        "            continue\n",
        "        seen_urls.add(url)\n",
        "        count += 1\n",
        "        title = (item.get(\"title\") or \"\").strip()\n",
        "        src = (item.get(\"source\") or \"\").strip()\n",
        "        when = (item.get(\"time_published\") or \"\").strip()\n",
        "        summ = (item.get(\"summary\") or \"\").strip()\n",
        "        sent_label = (item.get(\"overall_sentiment_label\") or \"\").strip()\n",
        "        sent_score = item.get(\"overall_sentiment_score\", \"\")\n",
        "        lines.append(\n",
        "            f\"[{count}] {title} — {src} — {when}\\n{url}\\n\"\n",
        "            f\"Summary: {summ}\\nSentiment: {sent_label} ({sent_score})\\n\"\n",
        "        )\n",
        "        if count >= max_items:\n",
        "            break\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_stock_data_av(\n",
        "    apikey: str,\n",
        "    ticker: str,\n",
        "    start_dt: Optional[datetime] = None,\n",
        "    end_dt: Optional[datetime] = None,\n",
        "    outputsize: str = 'compact' # 'compact' or 'full'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Fetch Alpha Vantage daily historical stock data.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY\",\n",
        "        \"symbol\": ticker,\n",
        "        \"outputsize\": outputsize,\n",
        "        \"apikey\": apikey,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(AV_BASE, params=params, timeout=25)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "\n",
        "        # Handle Alpha Vantage messages\n",
        "        if isinstance(js, dict) and any(k in js for k in (\"Note\", \"Information\", \"Error Message\")):\n",
        "            msg = js.get(\"Note\") or js.get(\"Information\") or js.get(\"Error Message\")\n",
        "            raise RuntimeError(f\"Alpha Vantage response: {msg}\")\n",
        "\n",
        "        # Extract daily time series data\n",
        "        time_series_data = js.get(\"Time Series (Daily)\", {}) or {}\n",
        "\n",
        "        if not time_series_data:\n",
        "            dprint(f\"No daily time series data found for ticker: {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to pandas DataFrame\n",
        "        df = pd.DataFrame.from_dict(time_series_data, orient='index')\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index() # Ensure chronological order\n",
        "\n",
        "        # Filter by date range if provided\n",
        "        if start_dt:\n",
        "            df = df[df.index >= start_dt.replace(tzinfo=None)]\n",
        "        if end_dt:\n",
        "            df = df[df.index <= end_dt.replace(tzinfo=None)]\n",
        "\n",
        "        dprint(f\"fetch_stock_data_av(ticker={ticker}) -> {len(df)} rows\")\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise RuntimeError(f\"API request failed: {e}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error processing stock data: {e}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def process_stock_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Processes historical stock data to extract relevant statistics.\"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Convert columns to numeric, forcing errors to NaN\n",
        "    for col in ['1. open', '2. high', '3. low', '4. close', '5. volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values after conversion\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Calculate daily price change and percentage change\n",
        "    df['daily_change'] = df['4. close'] - df['1. open']\n",
        "    df['daily_pct_change'] = (df['daily_change'] / df['1. open']) * 100\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['MA50'] = df['4. close'].rolling(window=50).mean()\n",
        "    df['MA200'] = df['4. close'].rolling(window=200).mean()\n",
        "\n",
        "    # Identify key values\n",
        "    latest_data = df.iloc[-1]\n",
        "    highest_close = df['4. close'].max()\n",
        "    lowest_close = df['4. close'].min()\n",
        "    date_highest_close = df['4. close'].idxmax()\n",
        "    date_lowest_close_val = df['4. close'].idxmin() # Corrected variable name\n",
        "\n",
        "    processed_info = {\n",
        "        \"latest_close\": latest_data['4. close'],\n",
        "        \"latest_open\": latest_data['1. open'],\n",
        "        \"latest_high\": latest_data['2. high'],\n",
        "        \"latest_low\": latest_data['3. low'],\n",
        "        \"latest_volume\": latest_data['5. volume'],\n",
        "        \"latest_daily_change\": latest_data['daily_change'],\n",
        "        \"latest_daily_pct_change\": latest_data['daily_pct_change'],\n",
        "        \"highest_close_price\": highest_close,\n",
        "        \"date_of_highest_close\": date_highest_close.strftime('%Y-%m-%d'),\n",
        "        \"lowest_close_price\": lowest_close,\n",
        "        \"date_of_lowest_close\": date_lowest_close_val.strftime('%Y-%m-%d'), # Using the corrected variable name\n",
        "        \"current_MA50\": latest_data['MA50'] if pd.notna(latest_data['MA50']) else None,\n",
        "        \"current_MA200\": latest_data['MA200'] if pd.notna(latest_data['MA200']) else None,\n",
        "        \"data_start_date\": df.index.min().strftime('%Y-%m-%d'),\n",
        "        \"data_end_date\": df.index.max().strftime('%Y-%m-%d'),\n",
        "        \"number_of_data_points\": len(df)\n",
        "    }\n",
        "\n",
        "    return processed_info\n",
        "\n",
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    # topics are not used for stock data, but keeping the extraction for potential future use\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch and process stock data\n",
        "    stock_data = {}\n",
        "    if tickers:\n",
        "        # Process only the first resolved ticker for simplicity in this step\n",
        "        try:\n",
        "            # Fetch full data to allow for MA calculations over longer periods\n",
        "            df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "            stock_data = process_stock_data(df)\n",
        "        except RuntimeError as e:\n",
        "            return f\"Stock data fetch or processing error: {e}\"\n",
        "    else:\n",
        "         return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "\n",
        "    if not stock_data or not stock_data.get(\"number_of_data_points\"):\n",
        "        return \"I couldn’t find sufficient historical stock data for that query. Try specifying a company name or ticker or a different date range.\"\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2)\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst answering questions based *only* on the provided historical stock data. \"\n",
        "        \"Do NOT use any external knowledge or refer to news articles. \"\n",
        "        \"Analyze the numerical data below to answer the user's question. \"\n",
        "        \"Include specific values and date ranges mentioned in the data where relevant. \"\n",
        "        \"If the requested information cannot be found in the data, state that clearly.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa4i-8IXvJIf",
        "outputId": "c14e8b4d-3351-4b4e-ce5c-7291a66012cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data for IBM:\n",
            "\n",
            "The latest available closing price for IBM is 243.49.\n",
            "On the latest trading day, the stock opened at 245.23, reached a high of 245.4599, and a low of 241.72. The daily change was -1.7399999999999807, representing a percentage change of -0.7095379847490033. The volume for this day was 2967558.0.\n",
            "\n",
            "Over the period from 2025-08-18 to 2025-08-29, the highest closing price recorded was 245.73 on 2025-08-28, and the lowest closing price was 239.4 on 2025-08-21.\n",
            "\n",
            "The 50-day Moving Average (MA50) and 200-day Moving Average (MA200) are not available in the provided data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify relevant api endpoint\n",
        "\n",
        "### Subtask:\n",
        "#### - Determine the appropriate Alpha Vantage API endpoint for historical stock data (e.g., TIME_SERIES_DAILY).\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - I need to determine the Alpha Vantage API endpoint for historical stock data. I will consult the Alpha Vantage API documentation to find the correct endpoint and its parameters.\n",
        "\n",
        "### Based on Alpha Vantage documentation, the TIME_SERIES_DAILY endpoint is used for historical daily stock data.\n",
        "#### - The required parameters are:\n",
        "#### - function: TIME_SERIES_DAILY\n",
        "#### - symbol: The ticker symbol of the stock (e.g., NVDA, AAPL)\n",
        "#### - outputsize: compact (last 100 data points) or full (full historical data)\n",
        "#### - apikey: Your Alpha Vantage API key\n",
        "#### - The response format is JSON and contains daily time series data with fields like '1. open', '2. high', '3. low', '4. close', '5. volume'.\n",
        "\n",
        "### Task\n",
        "#### - Create Python code using the Alpha Vantage API to fetch historical stock trend data and use the existing AI model to answer questions about the stock trends.\n",
        "\n",
        "### Test the updated function\n",
        "\n",
        "#### - Subtask:\n",
        "#### - Run the modified Q&A function with a sample question about stock trends to verify it uses the numerical data and provides relevant answers.\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - Call the modified `answer_company_news_question` function with a sample question about stock trends to verify its functionality.\n"
      ],
      "metadata": {
        "id": "YsyH2zdDvTML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the stock trend for IBM over the last 30 days?\"\n",
        "print(f\"Ask a company news question: {question}\\n\")\n",
        "print(\"Thinking...\\n\")\n",
        "answer = answer_company_news_question(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAxvy20EwAxZ",
        "outputId": "761e3dd7-8bfa-4c6b-e307-ba4cbe6de2ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: What is the stock trend for IBM over the last 30 days?\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data, I cannot determine the stock trend for IBM over the last 30 days. The available data only covers the period from \"2025-08-18\" to \"2025-08-29\", which is a span of 10 data points, not 30 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - The previous code failed because of a `NameError`. The variable `date_of_lowest_close` was not defined in the `process_stock_data` function. I need to fix this error in the `process_stock_data` function and re-run the function call.\n"
      ],
      "metadata": {
        "id": "8L5Jm-dDwFnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_stock_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Processes historical stock data to extract relevant statistics.\"\"\"\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Convert columns to numeric, forcing errors to NaN\n",
        "    for col in ['1. open', '2. high', '3. low', '4. close', '5. volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values after conversion\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        return {}\n",
        "\n",
        "    # Calculate daily price change and percentage change\n",
        "    df['daily_change'] = df['4. close'] - df['1. open']\n",
        "    df['daily_pct_change'] = (df['daily_change'] / df['1. open']) * 100\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['MA50'] = df['4. close'].rolling(window=50).mean()\n",
        "    df['MA200'] = df['4. close'].rolling(window=200).mean()\n",
        "\n",
        "    # Identify key values\n",
        "    latest_data = df.iloc[-1]\n",
        "    highest_close = df['4. close'].max()\n",
        "    lowest_close = df['4. close'].min()\n",
        "    date_highest_close = df['4. close'].idxmax()\n",
        "    date_lowest_close_val = df['4. close'].idxmin() # Corrected variable name\n",
        "\n",
        "    processed_info = {\n",
        "        \"latest_close\": latest_data['4. close'],\n",
        "        \"latest_open\": latest_data['1. open'],\n",
        "        \"latest_high\": latest_data['2. high'],\n",
        "        \"latest_low\": latest_data['3. low'],\n",
        "        \"latest_volume\": latest_data['5. volume'],\n",
        "        \"latest_daily_change\": latest_data['daily_change'],\n",
        "        \"latest_daily_pct_change\": latest_data['daily_pct_change'],\n",
        "        \"highest_close_price\": highest_close,\n",
        "        \"date_of_highest_close\": date_highest_close.strftime('%Y-%m-%d'),\n",
        "        \"lowest_close_price\": lowest_close,\n",
        "        \"date_of_lowest_close\": date_lowest_close_val.strftime('%Y-%m-%d'), # Using the corrected variable name\n",
        "        \"current_MA50\": latest_data['MA50'] if pd.notna(latest_data['MA50']) else None,\n",
        "        \"current_MA200\": latest_data['MA200'] if pd.notna(latest_data['MA200']) else None,\n",
        "        \"data_start_date\": df.index.min().strftime('%Y-%m-%d'),\n",
        "        \"data_end_date\": df.index.max().strftime('%Y-%m-%d'),\n",
        "        \"number_of_data_points\": len(df)\n",
        "    }\n",
        "\n",
        "    return processed_info\n",
        "\n",
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    # topics are not used for stock data, but keeping the extraction for potential future use\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch and process stock data\n",
        "    stock_data = {}\n",
        "    if tickers:\n",
        "        # Process only the first resolved ticker for simplicity in this step\n",
        "        try:\n",
        "            # Fetch full data to allow for MA calculations over longer periods\n",
        "            df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "            stock_data = process_stock_data(df)\n",
        "        except RuntimeError as e:\n",
        "            return f\"Stock data fetch or processing error: {e}\"\n",
        "    else:\n",
        "         return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "\n",
        "    if not stock_data or not stock_data.get(\"number_of_data_points\"):\n",
        "        return \"I couldn’t find sufficient historical stock data for that query. Try specifying a company name or ticker or a different date range.\"\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2)\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst answering questions based *only* on the provided historical stock data. \"\n",
        "        \"Do NOT use any external knowledge or refer to news articles. \"\n",
        "        \"Analyze the numerical data below to answer the user's question. \"\n",
        "        \"Include specific values and date ranges mentioned in the data where relevant. \"\n",
        "        \"If the requested information cannot be found in the data, state that clearly.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4KxiojevcRY",
        "outputId": "3c4d6a5c-4f34-4292-d595-402972a7a28d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data for IBM:\n",
            "\n",
            "*   **Latest Trading Day:**\n",
            "    *   The stock closed at $243.49.\n",
            "    *   It opened at $245.23.\n",
            "    *   The high for the day was $245.4599, and the low was $241.72.\n",
            "    *   The daily change was -$1.74, representing a -0.71% decrease.\n",
            "    *   The volume for the day was 2,967,558.0.\n",
            "\n",
            "*   **Historical Performance (from 2025-08-18 to 2025-08-29):**\n",
            "    *   The highest close price recorded was $245.73 on 2025-08-28.\n",
            "    *   The lowest close price recorded was $239.40 on 2025-08-21.\n",
            "\n",
            "*   **Moving Averages:**\n",
            "    *   The 50-day Moving Average (MA50) is not available in the provided data.\n",
            "    *   The 200-day Moving Average (MA200) is not available in the provided data.\n",
            "\n",
            "The data covers a period of 10 data points from 2025-08-18 to 2025-08-29.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - The previous attempt failed to identify a ticker from the question. I will call the `answer_company_news_question` function again, this time providing a clearer question that includes the ticker symbol, to ensure a ticker is identified and the stock data fetching and processing is attempted."
      ],
      "metadata": {
        "id": "BLgwx6kOwSwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the stock trend for IBM over the last 30 days?\"\n",
        "print(f\"Ask a company news question: {question}\\n\")\n",
        "print(\"Thinking...\\n\")\n",
        "answer = answer_company_news_question(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiM8cnctwYwn",
        "outputId": "1b22b8b0-eb3e-4f13-c472-93cbdd05d1b7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: What is the stock trend for IBM over the last 30 days?\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Based on the provided historical stock data, I cannot determine the stock trend for IBM over the last 30 days. The available data only covers the period from \"2025-08-18\" to \"2025-08-29\", which is a span of 10 data points, not 30 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A `NameError` was identified and corrected in the `process_stock_data` function related to handling the date of the lowest close price.\n",
        "*   The system successfully extracted the ticker symbol ex. \"NVDA\" or \"AAPL\" when it was explicitly provided in the user's question.\n",
        "*   The AI model utilized the provided numerical stock data to answer the question about the stock trend.\n",
        "*   The AI's response acknowledged the limitation of the available data range (10 days between \"2025-08-18\" and \"2025-08-29\") and based its analysis solely on this period, including details like the highest and lowest closing prices within this timeframe.\n",
        "*   The AI's response did not incorporate any information from news articles, confirming it adhered to the constraint of using only the provided numerical data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Implement more robust ticker symbol identification, potentially using a combination of company name search and direct ticker matching, to handle queries where the ticker is not explicitly provided.\n",
        "*   Enhance the AI's ability to summarize trends over limited data periods when the requested range is not fully available.\n"
      ],
      "metadata": {
        "id": "Rrb9h5wpwd8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task\n",
        "#### - Create a Python code using an API endpoint to pull stock trend data and combine it with news-based answers from an existing AI to provide a unified response to a user's question about a stock trend. The response should clearly show: a. answer based on news, b. answers based on numerical data, and c. combined answer.\n",
        "\n",
        "### Refactor data fetching\n",
        "\n",
        "### Subtask:\n",
        "#### - Modify the `answer_company_news_question` function to fetch both news articles and historical stock data for the identified ticker(s).\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - I need to modify the `answer_company_news_question` function to fetch both news articles and historical stock data. I will add calls to `fetch_news_av` and `fetch_stock_data_av` within the function, handling potential errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "vGHhCOR5whTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        return f\"News fetch error: {e}\"\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch full data to allow for MA calculations over longer periods\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without stock data\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {} # Ensure stock_data is empty if fetching fails\n",
        "\n",
        "\n",
        "    # Now, decide how to answer based on available data (news, stock, or both)\n",
        "    # This part will be modified in subsequent subtasks to combine answers.\n",
        "    # For now, we'll just indicate what data was found.\n",
        "\n",
        "    response_parts = []\n",
        "    if all_news:\n",
        "        response_parts.append(f\"Found {len(all_news)} relevant news articles.\")\n",
        "    else:\n",
        "        response_parts.append(\"No relevant news articles found.\")\n",
        "\n",
        "    if stock_data and stock_data.get(\"number_of_data_points\"):\n",
        "         response_parts.append(f\"Found {stock_data.get('number_of_data_points')} days of historical stock data.\")\n",
        "    else:\n",
        "         response_parts.append(\"No sufficient historical stock data found.\")\n",
        "\n",
        "    # Placeholder for the combined answer generation in a later step\n",
        "    # For now, just return the data availability status.\n",
        "    return \"\\n\".join(response_parts)\n",
        "\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr22d9wKwvxO",
        "outputId": "dd5717e4-a16c-43e9-f935-9610b35666ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Found 16 relevant news articles.\n",
            "Found 10 days of historical stock data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process both data types; text, data\n",
        "\n",
        "### Subtask:\n",
        "#### - Ensure both news articles and stock data are processed appropriately using the existing `build_context_snippets` and `process_stock_data` functions.\n",
        "\n",
        "### **Reasoning**:\n",
        "#### - Call the `build_context_snippets` function with the fetched news articles to generate a formatted string of news snippets and ensure the `process_stock_data` function is called with the fetched stock data, storing the results in variables for later use.\n"
      ],
      "metadata": {
        "id": "yiRmIvPqw0vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without news data\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = [] # Ensure all_news is empty if fetching fails\n",
        "\n",
        "    # Process news data\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch full data to allow for MA calculations over longer periods\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without stock data\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {} # Ensure stock_data is empty if fetching fails\n",
        "\n",
        "    # Now, decide how to answer based on available data (news, stock, or both)\n",
        "    # This part will be modified in subsequent subtasks to combine answers.\n",
        "    # For now, we'll just indicate what data was found and processed.\n",
        "\n",
        "    response_parts = []\n",
        "    if news_context:\n",
        "        response_parts.append(\"News articles processed.\")\n",
        "    else:\n",
        "        response_parts.append(\"No news articles processed.\")\n",
        "\n",
        "    if stock_data and stock_data.get(\"number_of_data_points\"):\n",
        "         response_parts.append(\"Historical stock data processed.\")\n",
        "    else:\n",
        "         response_parts.append(\"No historical stock data processed.\")\n",
        "\n",
        "    # Placeholder for the combined answer generation in a later step\n",
        "    # For now, just return the data availability status.\n",
        "    return \"\\n\".join(response_parts)\n",
        "\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmaEtOwvw_PD",
        "outputId": "d52989ca-a1e6-40b4-8add-993b7f474248"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "News articles processed.\n",
            "Historical stock data processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update ai prompt for combined context\n",
        "\n",
        "### Subtask:\n",
        "#### - Create a new AI prompt that includes both the formatted news snippets and the processed numerical stock data, instructing Gemini to provide a combined answer as well as separate news-based and data-based insights."
      ],
      "metadata": {
        "id": "SasFPP2PxFod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> str:\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return \"Could not identify a ticker for the query. Please specify a company name or ticker.\"\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without news data\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = [] # Ensure all_news is empty if fetching fails\n",
        "\n",
        "    # Process news data\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch full data to allow for MA calculations over longer periods\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        # This is not a fatal error, just report it and continue without stock data\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {} # Ensure stock_data is empty if fetching fails\n",
        "\n",
        "    # Format the numerical data for the prompt\n",
        "    data_context = json.dumps(stock_data, indent=2) if stock_data else \"No historical stock data available.\"\n",
        "\n",
        "    # Construct the combined prompt for Gemini\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst. Answer the user's question based *only* on the provided information (news articles and historical stock data).\\n\"\n",
        "        \"Do NOT use any external knowledge.\\n\"\n",
        "        \"Provide three distinct answers:\\n\"\n",
        "        \"1. News-Based Answer: Based *only* on the provided news articles. Cite articles using [citation number].\\n\"\n",
        "        \"2. Data-Based Answer: Based *only* on the provided historical stock data. Include specific values and date ranges mentioned in the data where relevant.\\n\"\n",
        "        \"3. Combined Answer: Synthesize insights from *both* the news articles and the historical stock data to provide a comprehensive answer to the user's question.\\n\\n\"\n",
        "        \"If the requested information cannot be found in either the news or the data, state that clearly in the relevant section.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{news_context if news_context else 'No news articles provided.'}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEMINI_MODEL,\n",
        "        contents=qa_prompt,  # pass a string\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.2,\n",
        "            max_output_tokens=900,\n",
        "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "        ),\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        print(answer_company_news_question(q))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dqZDQ5ZxJ9s",
        "outputId": "fe8d1fe8-5109-4bf6-c896-1b21085f1ad1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Here are three distinct answers regarding IBM, based solely on the provided information:\n",
            "\n",
            "### 1. News-Based Answer:\n",
            "\n",
            "IBM is highlighted as a stable income play and one of the best dividend stocks to buy now, alongside Coca-Cola, Altria, Cisco, and Realty Income [3]. It is also considered a \"Bull of the Day\" by Zacks Commentary, recognized as a bedrock of American innovation with deep R&D in the AI economy, fueling steady growth [7].\n",
            "\n",
            "The company is actively involved in the AI sector, with its watsonx platform, AI-driven automation, and strategic AI partnerships aiming to boost productivity and fuel enterprise transformation [9]. IBM is also tapping into AI-driven demand with strong growth prospects, estimate upgrades, and notable upside potential [4]. Furthermore, IBM and NASA launched Surya, an AI model designed to predict solar flares and protect critical infrastructure, improving flare prediction accuracy by 16% and offering two-hour advance visual forecasts [11].\n",
            "\n",
            "In the quantum computing space, IBM is a key player, backing quantum momentum alongside Microsoft and NVIDIA [5]. IBM and AMD have teamed up on quantum-centric supercomputing to merge quantum and classical systems for next-gen problem-solving, with the quantum market projected to soar to $97 billion by 2035 [6]. While IBM's hybrid cloud push is noted, Arista (ANET) shows a sharper growth edge in 2025 in the networking sector [12].\n",
            "\n",
            "IBM is also mentioned as one of the \"Ten Titans\" stocks already in the Dow Jones, with the potential for more megacap growth stocks to join by 2030 [2]. It is also listed among dividend stocks with reliable dividends, high yields, and solid growth prospects, with some yielding at least 3.5% [8, 10].\n",
            "\n",
            "### 2. Data-Based Answer:\n",
            "\n",
            "IBM's stock data covers the period from 2025-08-18 to 2025-08-29, with 10 data points. The latest available closing price is $243.49, with an open price of $245.23, a high of $245.4599, and a low of $241.72. On this latest day, the stock experienced a daily change of -$1.7399999999999807, representing a daily percentage change of -0.7095379847490033.\n",
            "\n",
            "During this period, IBM's highest close price was $245.73, recorded on 2025-08-28. The lowest close price was $239.40, observed on 2025-08-21. The 50-day and 200-day moving averages are not available in the provided data.\n",
            "\n",
            "### 3. Combined Answer:\n",
            "\n",
            "IBM is presented as a strong contender in the market, particularly due to its focus on AI and quantum computing, and its status as a reliable dividend stock. News articles highlight IBM's deep R&D in the AI economy, with its watsonx platform and strategic AI partnerships aiming to boost productivity and fuel enterprise transformation [7, 9]. The company is also actively involved in quantum computing, partnering with AMD on quantum-centric supercomputing and contributing to the projected $97 billion quantum market by 2035 [6]. IBM's innovative AI solutions extend to practical applications, such as the Surya AI model developed with NASA for solar flare prediction [11]. These initiatives suggest strong growth prospects and upside potential [4].\n",
            "\n",
            "From a financial stability perspective, IBM is identified as a stable income play and one of the best dividend stocks to buy, offering reliable dividends and high yields, with some yielding at least 3.5% [3, 8, 10]. It is also recognized as one of the \"Ten Titans\" already in the Dow Jones [2].\n",
            "\n",
            "Looking at the historical stock data from 2025-08-18 to 2025-08-29, IBM's stock has shown some fluctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - The previous response was truncated. I need to complete the output from the previous code block.\n"
      ],
      "metadata": {
        "id": "5vuDtgGRxUJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Answers a company news question by fetching news and stock data,\n",
        "    sending it to Gemini, and parsing the combined response into\n",
        "    news-based, data-based, and combined answers.\n",
        "    \"\"\"\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return {\"error\": \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"}\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return {\"error\": \"Could not identify a ticker for the query. Please specify a company name or ticker.\"}\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = []\n",
        "\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {}\n",
        "\n",
        "    data_context = json.dumps(stock_data, indent=2) if stock_data else \"No historical stock data available.\"\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst. Answer the user's question based *only* on the provided information (news articles and historical stock data).\\n\"\n",
        "        \"Do NOT use any external knowledge.\\n\"\n",
        "        \"Provide three distinct answers:\\n\"\n",
        "        \"1. News-Based Answer: Based *only* on the provided news articles. Cite articles using [citation number].\\n\"\n",
        "        \"2. Data-Based Answer: Based *only* on the provided historical stock data. Include specific values and date ranges mentioned in the data where relevant.\\n\"\n",
        "        \"3. Combined Answer: Synthesize insights from *both* the news articles and the historical stock data to provide a comprehensive answer to the user's question.\\n\\n\"\n",
        "        \"If the requested information cannot be found in either the news or the data, state that clearly in the relevant section.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{news_context if news_context else 'No news articles provided.'}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=qa_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=900,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        raw_response_text = resp.text\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error generating AI response: {e}\"}\n",
        "\n",
        "    # Parse the response\n",
        "    news_answer = \"Not found.\"\n",
        "    data_answer = \"Not found.\"\n",
        "    combined_answer = \"Not found.\"\n",
        "\n",
        "    # Use markers to split the response\n",
        "    news_marker = \"1. News-Based Answer:\"\n",
        "    data_marker = \"2. Data-Based Answer:\"\n",
        "    combined_marker = \"3. Combined Answer:\"\n",
        "\n",
        "    # Find the start of each section\n",
        "    news_start = raw_response_text.find(news_marker)\n",
        "    data_start = raw_response_text.find(data_marker)\n",
        "    combined_start = raw_response_text.find(combined_marker)\n",
        "\n",
        "    # Extract content based on markers\n",
        "    if news_start != -1:\n",
        "        news_content_start = news_start + len(news_marker)\n",
        "        if data_start != -1:\n",
        "            news_answer = raw_response_text[news_content_start:data_start].strip()\n",
        "        elif combined_start != -1:\n",
        "             news_answer = raw_response_text[news_content_start:combined_start].strip()\n",
        "        else:\n",
        "             news_answer = raw_response_text[news_content_start:].strip()\n",
        "\n",
        "\n",
        "    if data_start != -1:\n",
        "        data_content_start = data_start + len(data_marker)\n",
        "        if combined_start != -1:\n",
        "            data_answer = raw_response_text[data_content_start:combined_start].strip()\n",
        "        else:\n",
        "            data_answer = raw_response_text[data_content_start:].strip()\n",
        "\n",
        "\n",
        "    if combined_start != -1:\n",
        "        combined_content_start = combined_start + len(combined_marker)\n",
        "        combined_answer = raw_response_text[combined_content_start:].strip()\n",
        "\n",
        "    return {\n",
        "        \"news_based_answer\": news_answer,\n",
        "        \"data_based_answer\": data_answer,\n",
        "        \"combined_answer\": combined_answer\n",
        "    }\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        response = answer_company_news_question(q)\n",
        "        if \"error\" in response:\n",
        "            print(response[\"error\"])\n",
        "        else:\n",
        "            print(\"--- News-Based Answer ---\")\n",
        "            print(response[\"news_based_answer\"])\n",
        "            print(\"\\n--- Data-Based Answer ---\")\n",
        "            print(response[\"data_based_answer\"])\n",
        "            print(\"\\n--- Combined Answer ---\")\n",
        "            print(response[\"combined_answer\"])\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwQ69vAdxX3y",
        "outputId": "711cb509-1049-4055-a585-3f904610ae4f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "--- News-Based Answer ---\n",
            "IBM is highlighted as a stable income play and one of the best dividend stocks to buy now, alongside Coca-Cola, Altria, Cisco, and Realty Income [3]. It is also mentioned as a \"Bull of the Day\" by Zacks Commentary, recognized for its deep R&D in the AI economy, fueling steady growth [7]. IBM is actively tapping into AI-driven demand with strong growth prospects, estimate upgrades, and notable upside potential, particularly in computer-integrated systems with extensive AI applications [4].\n",
            "\n",
            "The company's focus on innovative AI solutions is expected to spur an uptrend, with its watsonx platform, AI-driven automation, and strategic AI partnerships aiming to boost productivity and fuel enterprise transformation [9]. IBM is also a key player in the quantum computing space, backing quantum momentum alongside Microsoft and NVIDIA [5]. Specifically, IBM and AMD have teamed up on quantum-centric supercomputing to merge quantum and classical systems for next-gen problem-solving, with the quantum market projected to soar to $97 billion by 2035 [6]. Furthermore, IBM, in collaboration with NASA, launched Surya, an AI model designed to predict solar flares and protect critical infrastructure, improving prediction accuracy by 16% and offering two-hour advance visual forecasts [11]. While IBM's hybrid cloud push is noted, Arista Networks (ANET) is seen as having a sharper growth edge in 2025 in the networking sector [12].\n",
            "\n",
            "###\n",
            "\n",
            "--- Data-Based Answer ---\n",
            "Over the period from August 18, 2025, to August 29, 2025, IBM's stock experienced fluctuations. The highest close price for IBM was $245.73 on August 28, 2025, while the lowest close price was $239.40 on August 21, 2025. The latest available data shows IBM's close price at $243.49, with an open price of $245.23, a high of $245.4599, and a low of $241.72. On this latest day, the stock experienced a daily change of -$1.74, representing a -0.71% decrease, with a volume of 2,967,558.\n",
            "\n",
            "###\n",
            "\n",
            "--- Combined Answer ---\n",
            "IBM is presented as a company with strong fundamentals and significant growth potential, particularly in the burgeoning AI and quantum computing sectors, while its stock has shown some recent volatility. News articles highlight IBM as a stable income play and a recommended dividend stock, recognized for its deep R&D in the AI economy and steady growth [3, 7]. The company is actively pursuing AI-driven demand through its watsonx platform, AI-driven automation, and strategic partnerships, which are expected to fuel enterprise transformation and an uptrend in its stock [4, 9]. IBM is also a major player in quantum computing, collaborating with AMD on quantum-centric supercomputing and backing quantum momentum alongside Microsoft and NVIDIA, with the quantum market projected for substantial growth [5, 6]. Additionally, IBM's innovative AI applications extend to areas like space weather prediction, as demonstrated by its Surya AI model developed with NASA [11].\n",
            "\n",
            "From a stock performance perspective, the provided data for August 18-29, 2025, shows that IBM's stock has experienced a range, with a high close of $245.73 on August 28, 2025, and a low close of $239.40 on August 21, 2025. The latest recorded close price was $243.49, which represented a daily decrease of -0.71% from its open. While the news suggests a positive outlook for IBM due to its strategic focus on AI and quantum computing, the recent stock data indicates some daily price fluctuations within the observed period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reasoning**:\n",
        "#### - Modify the print statements in the `__main__` block to use more descriptive headers and add clear visual separation between the sections, as well as an introductory sentence."
      ],
      "metadata": {
        "id": "UGMsmokixgOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_company_news_question(question: str, av_key: Optional[str] = None) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Answers a company news question by fetching news and stock data,\n",
        "    sending it to Gemini, and parsing the combined response into\n",
        "    news-based, data-based, and combined answers.\n",
        "    \"\"\"\n",
        "    av_key = av_key or os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
        "    if not av_key:\n",
        "        return {\"error\": \"Please set ALPHAVANTAGE_API_KEY (export ALPHAVANTAGE_API_KEY=...).\"}\n",
        "\n",
        "    intent = extract_intent_with_gemini(question)\n",
        "    companies = intent.get(\"companies\") or []\n",
        "    explicit_tickers = [t.strip().upper() for t in (intent.get(\"tickers\") or []) if t]\n",
        "    days = max(1, int(intent.get(\"days_lookback\") or 14))\n",
        "    topics = [str(t).lower().strip() for t in (intent.get(\"topics\") or []) if str(t).strip()]\n",
        "\n",
        "    # Build ticker list\n",
        "    tickers: List[str] = []\n",
        "    tickers.extend([t for t in explicit_tickers if t not in tickers])\n",
        "\n",
        "    # If user gave a company name (no ticker), resolve via SYMBOL_SEARCH\n",
        "    if not tickers and companies:\n",
        "        for c in companies:\n",
        "            for t in symbol_search_av(c, av_key):\n",
        "                if t not in tickers:\n",
        "                    tickers.append(t)\n",
        "\n",
        "    # Last-ditch: try searching the whole question as keywords\n",
        "    if not tickers:\n",
        "        for t in symbol_search_av(question, av_key):\n",
        "            if t not in tickers:\n",
        "                tickers.append(t)\n",
        "\n",
        "    dprint(\"resolved tickers:\", tickers or \"(none)\")\n",
        "\n",
        "    if not tickers:\n",
        "        return {\"error\": \"Could not identify a ticker for the query. Please specify a company name or ticker.\"}\n",
        "\n",
        "    # Time window\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days)\n",
        "\n",
        "    # Fetch news articles\n",
        "    all_news: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        if tickers:\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty, widen lookback to 60d and retry tickers once\n",
        "        if not all_news and tickers:\n",
        "            dprint(\"no news articles; widening window to 60 days\")\n",
        "            wider_start = end_dt - timedelta(days=60)\n",
        "            for t in tickers:\n",
        "                all_news.extend(fetch_news_av(av_key, ticker=t, start_dt=wider_start, end_dt=end_dt, topics=topics, limit=50))\n",
        "\n",
        "        # If still empty and we have topics, try topics-only\n",
        "        if not all_news and topics:\n",
        "            dprint(\"topics-only fallback for news\")\n",
        "            all_news = fetch_news_av(av_key, ticker=None, start_dt=start_dt, end_dt=end_dt, topics=topics, limit=50)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"News fetch error: {e}\")\n",
        "        all_news = []\n",
        "\n",
        "    news_context = build_context_snippets(all_news)\n",
        "\n",
        "    # Fetch and process stock data (using the first resolved ticker)\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        df = fetch_stock_data_av(av_key, ticker=tickers[0], start_dt=start_dt, end_dt=end_dt, outputsize='full')\n",
        "        stock_data = process_stock_data(df)\n",
        "    except RuntimeError as e:\n",
        "        dprint(f\"Stock data fetch or processing error: {e}\")\n",
        "        stock_data = {}\n",
        "\n",
        "    data_context = json.dumps(stock_data, indent=2) if stock_data else \"No historical stock data available.\"\n",
        "\n",
        "    qa_prompt = (\n",
        "        \"You are a financial analyst. Answer the user's question based *only* on the provided information (news articles and historical stock data).\\n\"\n",
        "        \"Do NOT use any external knowledge.\\n\"\n",
        "        \"Provide three distinct answers:\\n\"\n",
        "        \"1. News-Based Answer: Based *only* on the provided news articles. Cite articles using [citation number].\\n\"\n",
        "        \"2. Data-Based Answer: Based *only* on the provided historical stock data. Include specific values and date ranges mentioned in the data where relevant.\\n\"\n",
        "        \"3. Combined Answer: Synthesize insights from *both* the news articles and the historical stock data to provide a comprehensive answer to the user's question.\\n\\n\"\n",
        "        \"If the requested information cannot be found in either the news or the data, state that clearly in the relevant section.\\n\\n\"\n",
        "        f\"User question:\\n{question}\\n\\n\"\n",
        "        f\"Articles:\\n{news_context if news_context else 'No news articles provided.'}\\n\\n\"\n",
        "        f\"Historical Stock Data:\\n{data_context}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=GEMINI_MODEL,\n",
        "            contents=qa_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.2,\n",
        "                max_output_tokens=900,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "            ),\n",
        "        )\n",
        "        raw_response_text = resp.text\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error generating AI response: {e}\"}\n",
        "\n",
        "    # Parse the response\n",
        "    news_answer = \"Not found.\"\n",
        "    data_answer = \"Not found.\"\n",
        "    combined_answer = \"Not found.\"\n",
        "\n",
        "    # Use markers to split the response\n",
        "    news_marker = \"1. News-Based Answer:\"\n",
        "    data_marker = \"2. Data-Based Answer:\"\n",
        "    combined_marker = \"3. Combined Answer:\"\n",
        "\n",
        "    # Find the start of each section\n",
        "    news_start = raw_response_text.find(news_marker)\n",
        "    data_start = raw_response_text.find(data_marker)\n",
        "    combined_start = raw_response_text.find(combined_marker)\n",
        "\n",
        "    # Extract content based on markers\n",
        "    if news_start != -1:\n",
        "        news_content_start = news_start + len(news_marker)\n",
        "        if data_start != -1:\n",
        "            news_answer = raw_response_text[news_content_start:data_start].strip()\n",
        "        elif combined_start != -1:\n",
        "             news_answer = raw_response_text[news_content_start:combined_start].strip()\n",
        "        else:\n",
        "             news_answer = raw_response_text[news_content_start:].strip()\n",
        "\n",
        "\n",
        "    if data_start != -1:\n",
        "        data_content_start = data_start + len(data_marker)\n",
        "        if combined_start != -1:\n",
        "            data_answer = raw_response_text[data_content_start:combined_start].strip()\n",
        "        else:\n",
        "            data_answer = raw_response_text[data_content_start:].strip()\n",
        "\n",
        "\n",
        "    if combined_start != -1:\n",
        "        combined_content_start = combined_start + len(combined_marker)\n",
        "        combined_answer = raw_response_text[combined_content_start:].strip()\n",
        "\n",
        "    return {\n",
        "        \"news_based_answer\": news_answer,\n",
        "        \"data_based_answer\": data_answer,\n",
        "        \"combined_answer\": combined_answer\n",
        "    }\n",
        "\n",
        "# The __main__ block remains the same for user interaction\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        q = input(\"Ask a company news question: \")\n",
        "        print(\"\\nThinking...\\n\")\n",
        "        response = answer_company_news_question(q)\n",
        "        if \"error\" in response:\n",
        "            print(response[\"error\"])\n",
        "        else:\n",
        "            print(\"Here is the analysis based on the available information:\")\n",
        "            print(\"\\n--- News-Based Insights ---\")\n",
        "            print(response.get(\"news_based_answer\", \"N/A\"))\n",
        "            print(\"\\n--- Stock Data Analysis ---\")\n",
        "            print(response.get(\"data_based_answer\", \"N/A\"))\n",
        "            print(\"\\n--- Unified Trend Summary ---\")\n",
        "            print(response.get(\"combined_answer\", \"N/A\"))\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBk9vw6vxkl3",
        "outputId": "8bbe11a7-49e1-4e34-fc15-dfadce3fede0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a company news question: IBM\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Here is the analysis based on the available information:\n",
            "\n",
            "--- News-Based Insights ---\n",
            "IBM is highlighted as a stable income play and one of the best dividend stocks to buy now, alongside Coca-Cola, Altria, Cisco, and Realty Income [3]. It is also mentioned as a \"Bull of the Day\" due to its deep R&D in the AI economy, fueling steady growth [7]. IBM is actively tapping into AI-driven demand with strong growth prospects, estimate upgrades, and notable upside potential [4]. Its watsonx platform, AI-driven automation, and strategic AI partnerships are aimed at boosting productivity and fueling enterprise transformation [9].\n",
            "\n",
            "Furthermore, IBM is a key player in the quantum computing space, backing quantum momentum alongside Microsoft and NVIDIA [5]. It has teamed up with AMD on quantum-centric supercomputing to merge quantum and classical systems for next-gen problem-solving, with the quantum market projected to soar to $97 billion by 2035 [6]. IBM also collaborated with NASA to launch Surya, an AI model designed to predict solar flares and protect critical infrastructure, improving prediction accuracy by 16% and offering two-hour advance visual forecasts [11]. While IBM's hybrid cloud push is noted, Arista Networks (ANET) shows a sharper growth edge in 2025 in the networking sector [12].\n",
            "\n",
            "###\n",
            "\n",
            "--- Stock Data Analysis ---\n",
            "Based on the provided historical stock data from August 18, 2025, to August 29, 2025, IBM's latest closing price was $243.49, with an opening price of $245.23. The stock experienced a daily decrease of $1.74, or approximately -0.71%. During this period, IBM's highest closing price was $245.73, recorded on August 28, 2025. Its lowest closing price was $239.40, observed on August 21, 2025. The latest trading day saw a high of $245.4599 and a low of $241.72, with a volume of 2,967,558 shares.\n",
            "\n",
            "###\n",
            "\n",
            "--- Unified Trend Summary ---\n",
            "IBM is presented as a company with strong growth prospects, particularly in the AI and quantum computing sectors, while also being recognized as a stable dividend stock. News articles highlight IBM's deep R&D in the AI economy, its watsonx platform, and strategic AI partnerships aimed at enterprise transformation [4, 7, 9]. The company is also a significant player in quantum computing, collaborating with AMD on quantum-centric supercomputing and backing quantum momentum alongside Microsoft and NVIDIA [5, 6]. Additionally, IBM has innovated with NASA to launch an AI model, Surya, for solar flare prediction [11]. These positive developments suggest a company focused on future-oriented technologies and steady growth.\n",
            "\n",
            "From a stock performance perspective, the provided data for August 18-29, 2025, shows IBM's latest closing price at $243.49, experiencing a slight daily decline of approximately 0.71%. Over this short period, the stock reached its highest close of $245.73 on August 28, 2025, and its lowest close of $239.40 on August 21, 2025. While the news indicates a positive outlook for IBM as a dividend stock and a leader in AI and quantum computing, the recent stock data shows some daily fluctuation, including a slight decrease on the latest recorded day. This suggests that despite strong fundamental and strategic initiatives, the stock's short-term performance can still experience minor pullbacks.\n"
          ]
        }
      ]
    }
  ]
}
